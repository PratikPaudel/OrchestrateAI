import os
from typing import List, TypedDict, Annotated
import operator
import logging
import time

from langgraph.graph import StateGraph, END

# Import your agent classes
from app.agents.planner import PlannerAgent, ResearchPlan
from app.agents.searcher import SearcherAgent
from app.agents.summarizer import SummarizerAgent
from app.agents.reviewer import ReviewerAgent, Review
from app.agents.writer import WriterAgent

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("orchestrateai.graph")

# --- 1. Define the State for the Graph ---
# The state is a dictionary that will be passed between nodes.
# It holds all the information gathered during the research process.

class GraphState(TypedDict):
    """
    Represents the state of our research graph.

    Attributes:
        query: The initial user query.
        plan: The structured research plan from the PlannerAgent.
        tasks: A list of research tasks to be executed.
        current_task_index: The index of the task currently being processed.
        search_results: A list of search results for the current task.
        research_data: A list to accumulate all reviewed summaries from all tasks.
        final_report: The final report generated by the WriterAgent.
        error: A string to store error messages.
    """
    query: str
    plan: ResearchPlan
    tasks: List[str]
    current_task_index: int
    search_results: List[dict]
    
    # The 'operator.add' annotation tells LangGraph to append to this list
    # instead of overwriting it at each step.
    research_data: Annotated[List[dict], operator.add]
    
    final_report: str
    # Use operator.or_ to allow multiple nodes to set error messages
    # The first non-empty error will be used
    error: Annotated[str, operator.or_]


# --- 2. Instantiate Agents ---
# Create single instances of our agents to be used by the nodes.

planner_agent = PlannerAgent()
searcher_agent = SearcherAgent()
summarizer_agent = SummarizerAgent()
reviewer_agent = ReviewerAgent()
writer_agent = WriterAgent()


# --- 3. Define the Node Functions ---
# Each node in the graph is a function that takes the current state
# and returns a dictionary with the values to update in the state.

def planner_node(state: GraphState) -> dict:
    """
    Generates the initial research plan.
    """
    logger.info("--- ðŸ“ Executing Planner Node ---")
    try:
        query = state["query"]
        plan = planner_agent.create_plan(query)
        logger.info(f"Plan created with {len(plan.plan)} tasks.")
        return {
            "plan": plan,
            "tasks": plan.plan,
            "current_task_index": 0,
            "research_data": []
        }
    except Exception as e:
        logger.error(f"Planner node failed: {e}")
        return {"error": f"Planner node failed: {e}"}

def searcher_node(state: GraphState) -> dict:
    """
    Performs a web search for the current task.
    """
    logger.info(f"--- ðŸ” Executing Searcher Node for Task {state['current_task_index'] + 1} ---")
    try:
        # Check if we have tasks and if the current index is valid
        if not state.get("tasks") or state["current_task_index"] >= len(state["tasks"]):
            logger.error("No tasks available or invalid task index")
            return {"error": "No tasks available or invalid task index"}
        
        current_task = state["tasks"][state["current_task_index"]]
        logger.info(f"Searching for: {current_task}")
        search_results = searcher_agent.search(current_task)
        logger.info(f"Found {len(search_results)} search results.")
        return {"search_results": search_results}
    except Exception as e:
        logger.error(f"Searcher node failed: {e}")
        return {"error": f"Searcher node failed: {e}"}

def summarize_and_review_node(state: GraphState) -> dict:
    """
    Summarizes and reviews each search result for the current task.
    This node combines summarization and review for efficiency.
    """
    logger.info(f"--- ðŸ“– Executing Summarize & Review Node for Task {state['current_task_index'] + 1} ---")
    try:
        query = state["query"]
        search_results = state.get("search_results", [])
        
        if not search_results:
            logger.warning("No search results to process")
            return {
                "research_data": [],
                "current_task_index": state["current_task_index"] + 1
            }
        
        reviewed_summaries = []
        for i, result in enumerate(search_results):
            logger.info(f"    - Processing result {i+1}/{len(search_results)}: {result['url']}")
            
            # Add delay between processing each result
            if i > 0:
                logger.info(f"    - Waiting 3 seconds before processing next result...")
                time.sleep(3)
            
            logger.info(f"    - Summarizing URL: {result['url']}")
            summary = summarizer_agent.summarize(query, result["content"])
            
            logger.info(f"    - Reviewing Summary for: {result['url']}")
            review = reviewer_agent.review(summary, result["url"])
            
            if review.is_reliable:
                reviewed_summaries.append({
                    "url": result["url"],
                    "summary": summary,
                    "review": review.critique,
                })
                logger.info(f"    - Source accepted: {result['url']}")
            else:
                logger.warning(f"    - Discarding unreliable source: {result['url']}")

        # Update the overall research data with the findings from this task
        logger.info(f"Task {state['current_task_index'] + 1} complete. Waiting 5 seconds before next task...")
        time.sleep(5)  # Longer delay to respect rate limits
        return {
            "research_data": reviewed_summaries,
            "current_task_index": state["current_task_index"] + 1
        }
    except Exception as e:
        logger.error(f"Summarize & Review node failed: {e}")
        return {"error": f"Summarize & Review node failed: {e}"}

def writer_node(state: GraphState) -> dict:
    """
    Generates the final report from all the accumulated research data.
    """
    logger.info("--- âœï¸ Executing Writer Node ---")
    try:
        query = state["query"]
        research_data = state["research_data"]
        
        final_report = writer_agent.write_report(query, research_data)
        
        logger.info("Final report written.")
        return {"final_report": final_report}
    except Exception as e:
        logger.error(f"Writer node failed: {e}")
        return {"error": f"Writer node failed: {e}"}

def error_node(state: GraphState) -> dict:
    logger.error(f"Workflow halted due to error: {state.get('error', 'Unknown error')}")
    return {"final_report": f"ERROR: {state.get('error', 'Unknown error')}", "error": state.get('error', 'Unknown error')}

# --- 4. Define Conditional Logic ---
# This function decides which node to run next based on the current state.

def should_continue(state: GraphState) -> str:
    """
    Determines the next step after summarizing and reviewing a task's results.
    - If there are more tasks, loop back to the 'searcher' node.
    - If all tasks are complete, proceed to the 'writer' node.
    """
    logger.info("--- ðŸ¤” Checking for more tasks ---")
    if state.get("error"):
        logger.error(f"Error detected in state: {state['error']}")
        return "error"
    if state["current_task_index"] < len(state["tasks"]):
        logger.info("    - More tasks remaining. Looping back to searcher.")
        return "searcher"  # The name of the node to continue to
    else:
        logger.info("    - All tasks complete. Proceeding to writer.")
        return "writer" # The name of the node to continue to


# --- 5. Build the Graph ---
# Wire all the nodes and edges together into a state machine.

workflow = StateGraph(GraphState)

# Add nodes to the graph
workflow.add_node("planner", planner_node)
workflow.add_node("searcher", searcher_node)
workflow.add_node("summarize_and_review", summarize_and_review_node)
workflow.add_node("writer", writer_node)
workflow.add_node("error", error_node)

# Set the entry point of the graph
workflow.set_entry_point("planner")

# Add edges to define the flow
workflow.add_edge("planner", "searcher")
workflow.add_edge("searcher", "summarize_and_review")
workflow.add_edge("writer", END) # The writer node is the final step
workflow.add_edge("error", END)

# Add the conditional edge for the research loop
workflow.add_conditional_edges(
    "summarize_and_review",
    should_continue,
    {
        "searcher": "searcher",
        "writer": "writer",
        "error": "error",
    }
)

# Add conditional edges for error handling
workflow.add_conditional_edges(
    "planner",
    lambda state: "error" if state.get("error") else "searcher",
    {"searcher": "searcher", "error": "error"}
)

workflow.add_conditional_edges(
    "searcher",
    lambda state: "error" if state.get("error") else "summarize_and_review",
    {"summarize_and_review": "summarize_and_review", "error": "error"}
)

workflow.add_conditional_edges(
    "writer",
    lambda state: "error" if state.get("error") else END,
    {END: END, "error": "error"}
)

# Compile the graph into a runnable object
research_graph = workflow.compile()


# This allows you to run `python graph.py` to test the entire flow.
if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()

    # Define the initial input for the graph
    inputs = {
        "query": "Is solar power a viable alternative to fossil fuels in 2024?"
    }

    # Stream the graph's execution and print the output of each step
    for output in research_graph.stream(inputs, stream_mode="values"):
        # The 'stream_mode="values"' yields the entire state object at each step
        print("\n" + "="*80)
        print("CURRENT STATE:")
        # Find the last key that was updated in this step
        last_key = list(output.keys())[-1]
        print(f"Last updated field: '{last_key}'")
        print(output[last_key])
        print("="*80 + "\n")

    # The final state contains the report
    final_report = list(research_graph.stream(inputs, stream_mode="values"))[-1]['final_report']
    print("\n--- âœ… FINAL REPORT ---")
    print(final_report)